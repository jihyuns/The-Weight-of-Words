{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras, tensorflow\n",
    "keras.__version__\n",
    "tensorflow.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call the data of NSMC(Naver Sentiment Movie Corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        data = [line.split('\\t') for line in f.read().splitlines()]\n",
    "        data = data[1:]  # txt파일의 헤더(id label)는 제외\n",
    "    return data\n",
    "\n",
    "train_data = read_data('data/nsmcData/ratings_train.txt')\n",
    "test_data = read_data('data/nsmcData/ratings_test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the loaded data\n",
    "print(len(train_data))\n",
    "print(train_data[0])\n",
    "print(len(test_data))\n",
    "print(test_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing of NSMC data\n",
    "\n",
    "#### Using Okt(Open Korean Text) Class provided by KoNLPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "okt = Okt()\n",
    "print(okt.pos(u'나는 저 하늘을 높이 날고 있어 그때 니가 내게 줬던 두날개로'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "from pprint import pprint\n",
    "\n",
    "def tokenize(doc):\n",
    "    # norm은 정규화, stem은 근어로 표시\n",
    "    return ['/'.join(t) for t in okt.pos(doc, norm=True, stem=True)]\n",
    "\n",
    "if os.path.isfile('data/nsmcData/train_docs.json'):\n",
    "    with open('data/nsmcData/train_docs.json') as f:\n",
    "        train_docs = json.load(f)\n",
    "    with open('data/nsmcData/test_docs.json') as f:\n",
    "        test_docs = json.load(f)\n",
    "else:\n",
    "    train_docs = [(tokenize(row[1]), row[2]) for row in train_data]\n",
    "    test_docs = [(tokenize(row[1]), row[2]) for row in test_data]\n",
    "    \n",
    "    # save as json file\n",
    "    with open('data/nsmcData/train_docs.json', 'w', encoding='utf-8') as make_file:\n",
    "        json.dump(train_docs, make_file, ensure_ascii=False, indent=\"\\t\")\n",
    "    with open('data/nsmcData/test_docs.json', 'w', encoding='utf-8') as make_file:\n",
    "        json.dump(test_docs, make_file, ensure_ascii=False, indent=\"\\t\")\n",
    "        \n",
    "# Data pretty printer - pprint module\n",
    "pprint(train_docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of tokens in the analyzed data\n",
    "tokens = [t for d in  train_docs for t in d[0]]\n",
    "print(tokens[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pretreatment vis NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "text = nltk.Text(tokens, name='NSMC')\n",
    "\n",
    "print(len(text.tokens))        # Total tokens\n",
    "print(len(set(text.tokens)))   # Non-Duplicate tokens\n",
    "\n",
    "pprint(text.vocab().most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graph of 50 most common words using matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import font_manager, rc\n",
    "%matplotlib inline\n",
    "\n",
    "font_fname = '/Library/Fonts/AppleGothic.ttf'\n",
    "font_name = font_manager.FontProperties(fname=font_fname).get_name()\n",
    "rc('font', family=font_name)\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "text.plot(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorized using 10,000 commonly used token - Using CountVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_words = [f[0] for f in text.vocab().most_common(10000)]\n",
    "\n",
    "def term_frequency(doc):\n",
    "    return [doc.count(word) for word in selected_words]\n",
    "\n",
    "train_x = [term_frequency(d) for d, _ in train_docs]\n",
    "train_y = [c for _, c in train_docs]\n",
    "test_x = [term_frequency(d) for d, _ in test_docs]\n",
    "test_y = [c for _, c in test_docs]\n",
    "\n",
    "# Change data to float\n",
    "import numpy as np\n",
    "\n",
    "x_train = np.asarray(train_x).astype('float32')\n",
    "y_train = np.asarray(train_y).astype('float32')\n",
    "x_test = np.asarray(test_x).astype('float32')\n",
    "y_test = np.asarray(test_y).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition and Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer=optimizers.RMSprop(lr=0.001), loss=losses.binary_crossentropy, metrics=[metrics.binary_accuracy])\n",
    "model.fit(x_train, y_train, epochs=10, batch_size=512)\n",
    "results = model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict results with new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# df = pd.read_csv(\"data/commentsData/comments_Entertainment.csv\", sep=\",\")\n",
    "# df = pd.read_csv(\"data/commentsData/comments_Politics.csv\", sep=\",\")\n",
    "df = pd.read_csv(\"data/commentsData/comments_Social.csv\", sep=\",\")\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = []\n",
    "\n",
    "comments = df.comments\n",
    "\n",
    "print(comments[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict whether comment is pos(1) or neg(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_pos_neg(review):\n",
    "    token = tokenize(review)\n",
    "    tf = term_frequency(token)\n",
    "    data = np.expand_dims(np.asarray(tf).astype('float32'), axis=0)\n",
    "    score = float(model.predict(data))\n",
    "    \n",
    "    if(score>0.5):\n",
    "        print(\"[{}] is positive\\n\".format(review))\n",
    "        return 1\n",
    "    else:\n",
    "        print(\"[{}] is negative\\n\".format(review))\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the probability of pos or neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentage_pos_neg(review):\n",
    "    token = tokenize(review)\n",
    "    tf = term_frequency(token)\n",
    "    data = np.expand_dims(np.asarray(tf).astype('float32'), axis=0)\n",
    "    score = float(model.predict(data))\n",
    "    \n",
    "    if(score > 0.5):\n",
    "        print(\"[{}]는 {:.2f}% chance to be positive\\n\".format(review, score*100))\n",
    "        return round(score*100, 2)\n",
    "    else:\n",
    "        print(\"[{}]는 {:.2f}% chance to be negative\\n\".format(review, (1-score)*100))\n",
    "        return round((1-score)*100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = []\n",
    "\n",
    "for index in comments:\n",
    "    predict.append(predict_pos_neg(index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage = []\n",
    "\n",
    "for index in comments:\n",
    "    percentage.append(percentage_pos_neg(index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add column to csv file (predict / percent / truelike / wholelike)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['predict'] = predict\n",
    "df['percent'] = percentage\n",
    "\n",
    "truelike = df['like'] - df['dislike']\n",
    "wholelike = df['like'] + df['dislike']\n",
    "# truelike.head()\n",
    "# wholelike.head()\n",
    "\n",
    "df['truelike'] = truelike\n",
    "df['wholelike'] = wholelike\n",
    "\n",
    "df.to_csv('data/commentsData/comments_Social_.csv', mode='a', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sorting comment data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\"data/commentsData/comments_Entertainment_.csv\", sep=\",\")\n",
    "# df = pd.read_csv(\"data/commentsData/comments_Politics_.csv\", sep=\",\")\n",
    "df = pd.read_csv(\"data/commentsData/comments_Social_.csv\", sep=\",\")\n",
    "\n",
    "descendData = []\n",
    "ascendData = []\n",
    "total = []\n",
    "\n",
    "descendData = df.loc[df['predict']==1].sort_values(by='percent', ascending=False)\n",
    "ascendData = df.loc[df['predict']==0].sort_values(by='percent', ascending=True)\n",
    "\n",
    "total = descendData.append(ascendData)\n",
    "\n",
    "# total.to_csv('data/commentSorting/comments_Entertainment(sort).csv', mode='a', encoding='utf-8', index=False)\n",
    "# total.to_csv('data/commentSorting/comments_Politics(sort).csv', mode='a', encoding='utf-8', index=False)\n",
    "total.to_csv('data/commentSorting/comments_Social(sort).csv', mode='a', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the model as a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "from keras.models import model_from_json\n",
    "\n",
    "model.save('data/modelData/model.h5')\n",
    "\n",
    "# model_json = model.to_json()\n",
    "# with open('data/model.json', 'w') as json_file:\n",
    "#     json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras version: 2.2.2\n",
    "# tensorflow version: 1.11.0\n",
    "import tensorflowjs as tfjs\n",
    "tfjs.converters.save_keras_model(model, 'data/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
